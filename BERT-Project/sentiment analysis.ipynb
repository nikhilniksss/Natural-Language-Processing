{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Sentiment Analysis Project using BERT Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load BERT model\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "# download bert pretrained model\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased') # this model was trained on lowercase text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download Bert tokenizer\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# load Bert tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased',do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer Sequence [101, 3958, 27227, 2001, 1037, 13997, 11510, 102, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# input text\n",
    "\n",
    "text = \"Jim Henson was a puppeteer\"\n",
    "\n",
    "sent_id = tokenizer.encode(text,\n",
    "                        # add [CLS] and [SEP] tokens\n",
    "                        add_special_tokens = True,\n",
    "                        # specify maximum length \n",
    "                        max_length = 10,\n",
    "                        truncation = True,\n",
    "                        # add pad token to the right side of the token\n",
    "                        padding = 'max_length')\n",
    "\n",
    "# print integer sequence\n",
    "print(f\"Integer Sequence {sent_id}\")\n",
    "\n",
    "# 101 is [CLS] token in integer\n",
    "# 102 is [SEP] token in integer\n",
    "# last two 0's are the result of padding to make it max_length 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: ['[CLS]', 'jim', 'henson', 'was', 'a', 'puppet', '##eer', '[SEP]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# converting integer back to text as we can see 6 tokens generated for 5 words\n",
    "\n",
    "print(\"Tokenized Text:\",tokenizer.convert_ids_to_tokens(sent_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decode String [CLS] jim henson was a puppeteer [SEP] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# decode the tokenized text\n",
    "\n",
    "decode = tokenizer.decode(sent_id)\n",
    "print(\"Decode String {}\".format(decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Mask [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# mask to avoid performing attention on padding tokens\n",
    "# mask values : 1 is for that tokens that are NOT MASKED, 0 for MASKED tokens.\n",
    "\n",
    "att_mask = [int(tok > 0)for tok in sent_id]\n",
    "print(\"Attention Mask\",att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Understanding Input and Output***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  3958, 27227,  2001,  1037, 13997, 11510,   102,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "# convert list to tensors\n",
    "\n",
    "sent_id = torch.tensor(sent_id)\n",
    "att_mask = torch.tensor(att_mask)\n",
    "\n",
    "# reshaping tensor in the form of (batch,text length)\n",
    "\n",
    "sent_id = sent_id.unsqueeze(0)\n",
    "att_mask = att_mask.unsqueeze(0)\n",
    "\n",
    "# printing reshaped tensor\n",
    "print(sent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass integer sequence to the Bert Model\n",
    "\n",
    "outputs = bert(sent_id,att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapre of last hidden states: torch.Size([1, 10, 768])\n",
      "Shapre of CLS hidden states: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# unpack the output of Bert model\n",
    "\n",
    "all_hidden_states = outputs[0]\n",
    "\n",
    "cls_hidden_state = outputs[1]\n",
    "\n",
    "print(\"Shapre of last hidden states:\",all_hidden_states.shape)\n",
    "\n",
    "print(\"Shapre of CLS hidden states:\",cls_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8767, -0.4109, -0.1220,  0.4494,  0.1945, -0.2698,  0.8316,  0.3127,\n",
       "          0.1178, -1.0000, -0.1561,  0.6677,  0.9891, -0.3451,  0.8812, -0.6753,\n",
       "         -0.3079, -0.5580,  0.4380, -0.4588,  0.5831,  0.9956,  0.4467,  0.2863,\n",
       "          0.3924,  0.6864, -0.7513,  0.9043,  0.9436,  0.8207, -0.6493,  0.3524,\n",
       "         -0.9919, -0.2295, -0.0742, -0.9936,  0.3698, -0.7558,  0.0792, -0.2218,\n",
       "         -0.8637,  0.4711,  0.9997, -0.4368,  0.0404, -0.3498, -1.0000,  0.2663,\n",
       "         -0.8711,  0.0508,  0.0505, -0.1634,  0.1716,  0.4363,  0.4330, -0.0333,\n",
       "         -0.0416,  0.2206, -0.2568, -0.6122, -0.5916,  0.2569, -0.2622, -0.9041,\n",
       "          0.3221, -0.2394, -0.2634, -0.3454, -0.0723,  0.0081,  0.8297,  0.2279,\n",
       "          0.1614, -0.6555, -0.2062,  0.3280, -0.4016,  1.0000, -0.0952, -0.9874,\n",
       "         -0.0400,  0.0717,  0.3675,  0.3373, -0.3710, -1.0000,  0.4479, -0.1722,\n",
       "         -0.9917,  0.2677,  0.4844, -0.2207, -0.3207,  0.3715, -0.2171, -0.2522,\n",
       "         -0.3071, -0.3161, -0.1988, -0.0860, -0.0114, -0.1982, -0.1799, -0.3221,\n",
       "          0.1751, -0.4442, -0.1570, -0.0434, -0.0893,  0.5717,  0.3112, -0.2900,\n",
       "          0.3305, -0.9430,  0.6061, -0.2984, -0.9873, -0.3956, -0.9926,  0.7857,\n",
       "         -0.1692, -0.2719,  0.9505,  0.5628,  0.2904, -0.1693,  0.1619, -1.0000,\n",
       "         -0.1696, -0.1534,  0.2513, -0.2857, -0.9846, -0.9638,  0.5565,  0.9200,\n",
       "          0.1805,  0.9995, -0.2122,  0.9391,  0.3246, -0.3937, -0.1248, -0.5209,\n",
       "          0.0519,  0.1141, -0.6463,  0.3529, -0.0322, -0.3837, -0.3796, -0.2830,\n",
       "          0.1280, -0.9191, -0.4201,  0.9145,  0.0713, -0.2455,  0.5212, -0.2642,\n",
       "         -0.3675,  0.8082,  0.2577,  0.2755, -0.0157,  0.3675, -0.3107,  0.4502,\n",
       "         -0.8224,  0.2841,  0.4360, -0.3193,  0.2164, -0.9851, -0.4444,  0.5759,\n",
       "          0.9878,  0.7531,  0.3384,  0.2003, -0.2602,  0.4695, -0.9561,  0.9855,\n",
       "         -0.1712,  0.2295,  0.1220, -0.1386, -0.8436, -0.3783,  0.8371, -0.3204,\n",
       "         -0.8457, -0.0473, -0.4219, -0.3593, -0.2187,  0.5282, -0.3149, -0.4375,\n",
       "         -0.0440,  0.9242,  0.9296,  0.7735, -0.3733,  0.3945, -0.9049, -0.2898,\n",
       "          0.2695,  0.2910,  0.1695,  0.9932, -0.3069, -0.1611, -0.8349, -0.9827,\n",
       "          0.1299, -0.8555, -0.0531, -0.6830,  0.3926,  0.2873, -0.1899,  0.2598,\n",
       "         -0.9201, -0.7455,  0.3943, -0.3955,  0.4015, -0.2341,  0.7593,  0.3421,\n",
       "         -0.6143,  0.5170,  0.8987,  0.1072, -0.6858,  0.6481, -0.2454,  0.8712,\n",
       "         -0.5958,  0.9936,  0.3404,  0.4972, -0.9452, -0.2347, -0.8748, -0.0154,\n",
       "         -0.1293, -0.5265,  0.4235,  0.4206,  0.3663,  0.7488, -0.4650,  0.9900,\n",
       "         -0.8695, -0.9701, -0.5203, -0.0900, -0.9914,  0.0978,  0.2844, -0.0424,\n",
       "         -0.4649, -0.4546, -0.9620,  0.8035,  0.2177,  0.9705, -0.0793, -0.7985,\n",
       "         -0.3436, -0.9537, -0.0035, -0.0945,  0.4291,  0.0391, -0.9602,  0.4497,\n",
       "          0.5135,  0.4913,  0.0608,  0.9948,  1.0000,  0.9810,  0.8865,  0.7961,\n",
       "         -0.9894, -0.5122,  1.0000, -0.8521, -1.0000, -0.9412, -0.6633,  0.3110,\n",
       "         -1.0000, -0.1468, -0.1235, -0.9465, -0.0891,  0.9796,  0.9700, -1.0000,\n",
       "          0.9324,  0.9259, -0.4503,  0.4591, -0.1785,  0.9819,  0.2285,  0.4423,\n",
       "         -0.2615,  0.4124, -0.5252, -0.8534,  0.0365, -0.0670,  0.8944,  0.1913,\n",
       "         -0.4782, -0.9402,  0.2293, -0.1581, -0.2440, -0.9604, -0.1924, -0.0555,\n",
       "          0.5484,  0.1915,  0.2038, -0.7367,  0.2698, -0.7307,  0.3715,  0.5640,\n",
       "         -0.9386, -0.5717,  0.3818, -0.2775,  0.1536, -0.9608,  0.9702, -0.3502,\n",
       "          0.1524,  1.0000,  0.3876, -0.9001,  0.2547,  0.1857,  0.0832,  1.0000,\n",
       "          0.3811, -0.9852, -0.4053,  0.2576, -0.3923, -0.4125,  0.9994, -0.1463,\n",
       "         -0.0428,  0.2818,  0.9899, -0.9923,  0.8351, -0.8563, -0.9634,  0.9617,\n",
       "          0.9268, -0.4225, -0.7369,  0.1318,  0.1107,  0.2294, -0.8914,  0.6082,\n",
       "          0.4665, -0.0720,  0.8555, -0.7973, -0.3478,  0.4201, -0.1762,  0.0761,\n",
       "          0.2823,  0.4571, -0.1350,  0.1190, -0.3509, -0.4039, -0.9556,  0.0262,\n",
       "          1.0000, -0.2164,  0.0569, -0.2296, -0.1003, -0.1827,  0.4036,  0.4715,\n",
       "         -0.3293, -0.8471, -0.0518, -0.8453, -0.9935,  0.6732,  0.2284, -0.1968,\n",
       "          0.9998,  0.5194,  0.2326,  0.1718,  0.7497, -0.0192,  0.4518, -0.0327,\n",
       "          0.9765, -0.3259,  0.3491,  0.7471, -0.3186, -0.3019, -0.5725,  0.0563,\n",
       "         -0.9206,  0.0572, -0.9589,  0.9565,  0.3109,  0.3348,  0.1635, -0.0619,\n",
       "          1.0000, -0.6020,  0.5309, -0.3723,  0.6636, -0.9851, -0.6789, -0.4312,\n",
       "         -0.1435, -0.0827, -0.2497,  0.1323, -0.9786, -0.0474, -0.0304, -0.9444,\n",
       "         -0.9927,  0.2508,  0.6172,  0.1679, -0.7980, -0.6078, -0.4906,  0.4646,\n",
       "         -0.1934, -0.9396,  0.5453, -0.3000,  0.4329, -0.3340,  0.4408, -0.2058,\n",
       "          0.8344,  0.1265, -0.0307, -0.2098, -0.8340,  0.7114, -0.7410,  0.0518,\n",
       "         -0.1481,  1.0000, -0.3100,  0.1461,  0.7011,  0.6334, -0.2857,  0.1618,\n",
       "          0.0966,  0.2955, -0.0981, -0.1832, -0.6208, -0.3013,  0.4337,  0.0283,\n",
       "         -0.2959,  0.7579,  0.4711,  0.3666, -0.0531,  0.0914,  0.9969, -0.2267,\n",
       "         -0.1165, -0.5533, -0.1262, -0.3575, -0.2124,  1.0000,  0.3679,  0.0604,\n",
       "         -0.9936, -0.2000, -0.9208,  0.9999,  0.8511, -0.8783,  0.5650,  0.2405,\n",
       "         -0.2859,  0.6935, -0.2598, -0.2655,  0.2893,  0.2862,  0.9774, -0.4575,\n",
       "         -0.9764, -0.5964,  0.3966, -0.9575,  0.9939, -0.5326, -0.2349, -0.4376,\n",
       "         -0.0250,  0.2574,  0.0274, -0.9762, -0.1582,  0.1821,  0.9811,  0.3014,\n",
       "         -0.3820, -0.9007, -0.1151,  0.3936, -0.0680, -0.9449,  0.9809, -0.9313,\n",
       "          0.2600,  1.0000,  0.3860, -0.5243,  0.2401, -0.4410,  0.3253, -0.1413,\n",
       "          0.5428, -0.9466, -0.2817, -0.3262,  0.4330, -0.2120, -0.2457,  0.7247,\n",
       "          0.2134, -0.3430, -0.6305, -0.1214,  0.4871,  0.7498, -0.2957, -0.1829,\n",
       "          0.1699, -0.1391, -0.9264, -0.4167, -0.2995, -0.9991,  0.6411, -1.0000,\n",
       "         -0.1510, -0.5473, -0.2219,  0.8075,  0.3862, -0.1392, -0.7206, -0.0710,\n",
       "          0.6995,  0.6656, -0.2889,  0.2902, -0.6951,  0.1622, -0.1298,  0.3182,\n",
       "          0.1694,  0.6526, -0.2735,  1.0000,  0.1370, -0.3043, -0.9189,  0.3041,\n",
       "         -0.2604,  1.0000, -0.7969, -0.9715,  0.2110, -0.5773, -0.7218,  0.2477,\n",
       "         -0.0304, -0.7015, -0.6577,  0.9111,  0.8219, -0.3693,  0.4537, -0.3062,\n",
       "         -0.3671,  0.0856,  0.1595,  0.9903,  0.2790,  0.8213, -0.2885, -0.0724,\n",
       "          0.9636,  0.2213,  0.6892,  0.2070,  1.0000,  0.3249, -0.8999,  0.2644,\n",
       "         -0.9700, -0.2610, -0.9228,  0.4016,  0.1170,  0.8570, -0.3587,  0.9672,\n",
       "          0.0667,  0.1108, -0.1840,  0.4711,  0.3127, -0.9391, -0.9892, -0.9908,\n",
       "          0.3962, -0.5013, -0.0640,  0.3811,  0.1530,  0.4712,  0.3781, -1.0000,\n",
       "          0.9466,  0.3529,  0.2077,  0.9735,  0.2019,  0.4726,  0.4248, -0.9892,\n",
       "         -0.9203, -0.3418, -0.2910,  0.6572,  0.5584,  0.8190,  0.4319, -0.4171,\n",
       "         -0.4697,  0.4653, -0.8583, -0.9940,  0.4802,  0.0740, -0.8986,  0.9559,\n",
       "         -0.4745, -0.1616,  0.4457,  0.1412,  0.8933,  0.8280,  0.4313,  0.2437,\n",
       "          0.6787,  0.9043,  0.8940,  0.9903, -0.2561,  0.6986, -0.0055,  0.3281,\n",
       "          0.6809, -0.9586,  0.1583,  0.0033, -0.2711,  0.3025, -0.1928, -0.9207,\n",
       "          0.5260, -0.2139,  0.5709, -0.2302,  0.1593, -0.4779, -0.1577, -0.7036,\n",
       "         -0.5208,  0.4676,  0.2335,  0.9372,  0.4775, -0.1995, -0.5655, -0.2336,\n",
       "          0.0798, -0.9315,  0.8288, -0.0946,  0.5294,  0.0223, -0.0744,  0.7821,\n",
       "          0.1236, -0.3705, -0.3959, -0.7528,  0.8145, -0.3204, -0.4786, -0.5135,\n",
       "          0.7306,  0.3208,  0.9981, -0.3959, -0.3492, -0.1118, -0.2872,  0.3596,\n",
       "         -0.1345, -1.0000,  0.2896,  0.2262,  0.1702, -0.3530,  0.1111, -0.0755,\n",
       "         -0.9565, -0.2658,  0.2530, -0.0490, -0.5834, -0.4616,  0.3937,  0.2329,\n",
       "          0.5620,  0.8138, -0.0288,  0.5621,  0.3811,  0.0852, -0.6049,  0.8452]],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Data Preparation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "0                                                                                             @VirginAmerica What @dhepburn said.   \n",
       "1                                                        @VirginAmerica plus you've added commercials to the experience... tacky.   \n",
       "2                                                         @VirginAmerica I didn't today... Must mean I need to take another trip!   \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4                                                                         @VirginAmerica and it's a really big bad thing about it   \n",
       "\n",
       "  tweet_coord              tweet_created tweet_location  \\\n",
       "0         NaN  2015-02-24 11:35:52 -0800            NaN   \n",
       "1         NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "2         NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
       "3         NaN  2015-02-24 11:15:36 -0800            NaN   \n",
       "4         NaN  2015-02-24 11:14:45 -0800            NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0  Eastern Time (US & Canada)  \n",
       "1  Pacific Time (US & Canada)  \n",
       "2  Central Time (US & Canada)  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tweet.csv\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth',200)\n",
    "\n",
    "# read csv file\n",
    "\n",
    "tweets = pd.read_csv(r\"/Users/nick_mac/Desktop/Natural-Language-Processing/BERT-Project/data/Tweets.csv\")\n",
    "\n",
    "# print top 5 rows\n",
    "\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check the dataframe\n",
    "\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5156                                                                                      @SouthwestAir I'll do that. Can't DM until you follow me. Thanks!\n",
       "13091                                  @AmericanAir don't they already know ?  Isn't everyone sharing how nasty the food is? It's not even close to decent.\n",
       "10873    @USAirways Cancelled Flightled flight, 50 person line and one agent helping to rebook. You could've handed this one better. http://t.co/By3vDioSUA\n",
       "6346                              @SouthwestAir CEO discusses possibilities of NHL Vegas arena naming rights http://t.co/t3E4SH5xNg @SMUSportMgt #sportsbiz\n",
       "13814                                                                                                                            @AmericanAir ticks me off.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly sample of tweets\n",
    "\n",
    "tweets['text'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment\n",
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment class distribution\n",
    "\n",
    "tweets['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment\n",
       "negative    0.626913\n",
       "neutral     0.211680\n",
       "positive    0.161407\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the proportion\n",
    "\n",
    "tweets['airline_sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9178, 3099, 2363]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the value counts to a list\n",
    "\n",
    "class_count = tweets['airline_sentiment'].value_counts().to_list()\n",
    "class_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Text Cleaning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'@[A-Za-z0-9]+','',text)\n",
    "    text = re.sub(r'http\\S+','',text)\n",
    "    tokens = text.split()\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform text cleaning\n",
    "\n",
    "tweets['clean_text'] = tweets['text'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned text and labels to the variable\n",
    "\n",
    "text = tweets['clean_text'].values\n",
    "labels = tweets['airline_sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"plus you've added commercials to the experience... tacky.\",\n",
       "       \"i didn't today... must mean i need to take another trip!\",\n",
       "       'it\\'s really aggressive to blast obnoxious \"entertainment\" in your guests\\' faces &amp; they have little recourse',\n",
       "       \"and it's a really big bad thing about it\",\n",
       "       \"seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying va\",\n",
       "       'yes, nearly every time i fly vx this “ear worm” won’t go away :)',\n",
       "       'really missed a prime opportunity for men without hats parody, there.',\n",
       "       \"well, i didn't…but now i do! :-d\",\n",
       "       \"it was amazing, and arrived an hour early. you're too good to me.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaned text\n",
    "\n",
    "text[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Preparing Input and Output Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing label encode\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# define label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# fit and transform\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'neutral' 'positive']\n",
      "[1 2 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# classes\n",
    "\n",
    "print(le.classes_)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Length of sentences')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# printing the sequence of the tweets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num = [len(i.split()) for i in text]\n",
    "\n",
    "plt.hist(num,bins= 30)\n",
    "\n",
    "plt.title(\"Length of sentences\")\n",
    "\n",
    "# the most of the tweets hasmax length of 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define maximum length of a text\n",
    "\n",
    "max_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 14640/14640 [00:00<00:00, 18802.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# library for progress bar\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# create an empty list to save integer sequence\n",
    "\n",
    "sent_id = []\n",
    "\n",
    "# iterate over each tweet\n",
    "\n",
    "for i in tqdm(range(len(text))):\n",
    "    encoded_sent = tokenizer.encode(text[i],\n",
    "                                    add_special_tokens = True,\n",
    "                                    max_length = max_len,\n",
    "                                    truncation = True,\n",
    "                                    padding = 'max_length')\n",
    "    \n",
    "    sent_id.append(encoded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer Sequence [101, 2054, 2056, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print (\"Integer Sequence\",sent_id[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "for sent in sent_id:\n",
    "    att_mask = [int(token_id) > 0 for token_id in sent]\n",
    "\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Training and Validation Dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(sent_id,labels, random_state= 2018,test_size= 0.1,stratify=labels)\n",
    "\n",
    "train_masks, validation_masks, _,_ = train_test_split(attention_masks,labels,random_state= 2018,test_size= 0.1,stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Define Data loader***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler\n",
    "\n",
    "# define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Dataset wrapping tensors\n",
    "train_data = TensorDataset(train_inputs,train_masks,train_labels)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_data,sampler = train_sampler,batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs,validation_masks,validation_labels)\n",
    "\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "\n",
    "validation_dataloader = DataLoader(validation_data,sampler = validation_sampler,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an iterator object\n",
    "iterator = iter(train_dataloader)\n",
    "\n",
    "# loads batch data\n",
    "sent_id, mask, target = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 25])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass inputs to the model\n",
    "outputs = bert(sent_id,attention_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 25, 768])\n",
      "torch.Size([32, 768])\n"
     ]
    }
   ],
   "source": [
    "# hidden states\n",
    "hidden_states = outputs[0]\n",
    "\n",
    "# [CLS] hidden state\n",
    "CLS_hidden_state = outputs[1]\n",
    "\n",
    "print(hidden_states.shape)\n",
    "print(CLS_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Model Finetuning***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to follow while finetuning Head only:\n",
    "\n",
    "1. Turn off Gradient, means freeze the Bert parameters\n",
    "2. Define Model Architecture\n",
    "3. Define Optimizer and Loss function\n",
    "4. Define Traine and Validations set\n",
    "5. Train the model\n",
    "6. Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1.Turn Off Gradient***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2.Define Model Architecture***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class classifier(nn.Module):\n",
    "\n",
    "    def __init__(self,bert):\n",
    "\n",
    "        super(classifier,self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "\n",
    "        # dense layer 2\n",
    "        self.fc2 = nn.Linear(512,3)\n",
    "\n",
    "        #dropout layer (output layer)\n",
    "        self.dropout= nn.Dropout(0.1)\n",
    "\n",
    "        # Activation Function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        # pass the input to the model\n",
    "        outputs = self.bert(input_ids=sent_id, attention_mask=mask)\n",
    "\n",
    "        # use pooler_output as the [CLS] token representation\n",
    "        cls_hidden_state = outputs.pooler_output\n",
    "\n",
    "        # pass CLS hidden state to the dense layer\n",
    "        x = self.fc1(cls_hidden_state)\n",
    "\n",
    "        # Apply ReLU activation function\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # pass input to the output layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # apply softmax\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = classifier(bert)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_id = sent_id.to(device)\n",
    "mask = mask.to(device)\n",
    "target = target.to(device)\n",
    "\n",
    "outputs = model(sent_id,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1624, -1.1286, -1.0112],\n",
      "        [-1.2495, -1.0671, -0.9960],\n",
      "        [-1.1694, -1.0446, -1.0858],\n",
      "        [-1.2061, -1.0349, -1.0631],\n",
      "        [-1.1935, -0.9883, -1.1251],\n",
      "        [-1.1154, -1.0898, -1.0909],\n",
      "        [-1.2971, -1.0167, -1.0082],\n",
      "        [-1.2236, -1.0502, -1.0329],\n",
      "        [-1.2516, -0.9879, -1.0741],\n",
      "        [-1.2046, -1.0808, -1.0192],\n",
      "        [-1.1865, -1.0489, -1.0660],\n",
      "        [-1.1057, -1.1272, -1.0640],\n",
      "        [-1.2885, -1.0379, -0.9939],\n",
      "        [-1.1669, -1.0772, -1.0552],\n",
      "        [-1.2341, -1.0049, -1.0705],\n",
      "        [-1.2064, -1.0982, -1.0017],\n",
      "        [-1.0878, -1.1586, -1.0524],\n",
      "        [-1.1506, -1.1345, -1.0162],\n",
      "        [-1.3030, -1.0295, -0.9914],\n",
      "        [-1.1649, -1.0362, -1.0989],\n",
      "        [-1.2232, -1.0523, -1.0312],\n",
      "        [-1.1489, -1.1104, -1.0396],\n",
      "        [-1.2412, -1.0431, -1.0255],\n",
      "        [-1.2425, -0.9975, -1.0714],\n",
      "        [-1.1862, -1.1507, -0.9724],\n",
      "        [-1.2368, -1.0406, -1.0316],\n",
      "        [-1.1939, -1.0280, -1.0811],\n",
      "        [-1.2689, -0.9744, -1.0746],\n",
      "        [-1.2779, -1.0746, -0.9678],\n",
      "        [-1.2176, -1.1150, -0.9778],\n",
      "        [-1.2737, -1.0410, -1.0021],\n",
      "        [-1.1957, -1.0886, -1.0193]], device='mps:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model has 395,267 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# to get the number of trainable parameter\n",
    "\n",
    "def count_parameter(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'the model has {count_parameter(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3.Define optimizer and Loss function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam optimizer\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'class Distributions')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# understand the class distribution\n",
    "keys = ['0','1','2']\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.bar(keys,class_count)\n",
    "\n",
    "plt.title('class Distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weight: [0.53170625 1.57470152 2.06517139]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "\n",
    "print('class weight:', class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "weights = weights.to(device)\n",
    "\n",
    "cross_entropy = nn.NLLLoss(weight = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0893, device='mps:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# compute loss \n",
    "\n",
    "loss = cross_entropy(outputs,target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***4.Model Training and Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(\"\\ntraining.....\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # initialize loss and accuracy to 0\n",
    "    total_loss,total_accuracy = 0,0\n",
    "\n",
    "    total_preds= []\n",
    "\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            print(' Batch {:>5,} of {:>5,}. Elapsed: {:}'.format(step,len(train_dataloader),elapsed))\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        sent_id,mask,labels = batch\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        pred = model(sent_id,mask)\n",
    "\n",
    "        loss = cross_entropy(pred,labels)\n",
    "\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    avg_loss = total_loss/len(train_dataloader)\n",
    "\n",
    "    total_preds = np.concatenate(total_preds,axis = 0)\n",
    "\n",
    "    return avg_loss,total_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optional: helper to format elapsed time\n",
    "def format_time(elapsed):\n",
    "    return str(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed)))\n",
    "\n",
    "# Define a function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\nEvaluating.....\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_preds = []\n",
    "\n",
    "    # For each batch in validation data\n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "\n",
    "        # Progress update\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(validation_dataloader), elapsed))\n",
    "\n",
    "        # Move batch to GPU if available\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get predictions\n",
    "            outputs = model(sent_id, mask)\n",
    "\n",
    "            # If model returns a tuple (like BERT), extract logits\n",
    "            if isinstance(outputs, tuple):\n",
    "                logits = outputs[0]\n",
    "            else:\n",
    "                logits = outputs\n",
    "\n",
    "            # Calculate loss (you must define `loss_fn`)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Move logits to CPU and convert to numpy\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # Compute average loss\n",
    "    avg_loss = total_loss / len(validation_dataloader)\n",
    "\n",
    "    # Stack predictions\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "........... epoch 1 / 5 ..........\n",
      "\n",
      "training.....\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'preds' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m........... epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m ..........\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, epochs))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m train_loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     21\u001b[0m valid_loss, _ \u001b[38;5;241m=\u001b[39m evaluate()\n",
      "Cell \u001b[0;32mIn[62], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     35\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 37\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mpreds\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     39\u001b[0m     total_preds\u001b[38;5;241m.\u001b[39mappend(preds)\n\u001b[1;32m     41\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'preds' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assign the initial loss to infinity\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# Create empty lists to store training and validation loss of each epoch\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 5\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(epochs):\n",
    "    print('\\n........... epoch {} / {} ..........'.format(epoch + 1, epochs))\n",
    "\n",
    "    # Train the model\n",
    "    train_loss, _ = train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    valid_loss, _ = evaluate()\n",
    "\n",
    "    # Save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "\n",
    "    # Accumulate training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    # Print loss for current epoch\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
