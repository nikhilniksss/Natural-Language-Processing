{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>NLP Data Preprocessing</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import important libraries\n",
    "\n",
    "import ssl\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nikhilsingh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nikhilsingh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nikhilsingh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nikhilsingh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download predefined dictionary\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The global economy is facing a challenging period. In April 2023, the International Monetary Fund (IMF) projected global growth to slow to 2.8%,\n",
       "down from 3.4% in 2022. Rising inflation, geopolitical tensions, and supply chain disruptions have added to the uncertainty. Major tech companies like Google,\n",
       "Amazon, and Microsoft have responded by cutting jobs and scaling back investments. Meanwhile, emerging markets such as India and Brazil are showing resilience. \n",
       "Artificial Intelligence (AI) is becoming a key area of innovation and competition among nations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The text with whic we process\n",
    "\n",
    "text = \"\"\"The global economy is facing a challenging period. In April 2023, the International Monetary Fund (IMF) projected global growth to slow to 2.8%,\n",
    "down from 3.4% in 2022. Rising inflation, geopolitical tensions, and supply chain disruptions have added to the uncertainty. Major tech companies like Google,\n",
    "Amazon, and Microsoft have responded by cutting jobs and scaling back investments. Meanwhile, emerging markets such as India and Brazil are showing resilience. \n",
    "Artificial Intelligence (AI) is becoming a key area of innovation and competition among nations.\"\"\"\n",
    "\n",
    "display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "the global economy is facing a challenging period. in april 2023, the international monetary fund (imf) projected global growth to slow to 2.8%,\n",
       "down from 3.4% in 2022. rising inflation, geopolitical tensions, and supply chain disruptions have added to the uncertainty. major tech companies like google,\n",
       "amazon, and microsoft have responded by cutting jobs and scaling back investments. meanwhile, emerging markets such as india and brazil are showing resilience. \n",
       "artificial intelligence (ai) is becoming a key area of innovation and competition among nations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's start preprocessing\n",
    "\n",
    "text = text.lower() # lower the case\n",
    "display(Markdown(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Bag of words***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'This is for testing statement',\n",
    "    'I love playing cricket',\n",
    "    'I have played lot of matches'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cricket: 0\n",
      "for: 1\n",
      "have: 2\n",
      "is: 3\n",
      "lot: 4\n",
      "love: 5\n",
      "matches: 6\n",
      "of: 7\n",
      "played: 8\n",
      "playing: 9\n",
      "statement: 10\n",
      "testing: 11\n",
      "this: 12\n"
     ]
    }
   ],
   "source": [
    "vectorizer.vocabulary_\n",
    "\n",
    "sorted_vocab_by_index = sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])\n",
    "for word, index in sorted_vocab_by_index:\n",
    "    print(f\"{word}: {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TF-IDF***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cricket: 0\n",
      "for: 1\n",
      "have: 2\n",
      "is: 3\n",
      "lot: 4\n",
      "love: 5\n",
      "matches: 6\n",
      "of: 7\n",
      "played: 8\n",
      "playing: 9\n",
      "statement: 10\n",
      "testing: 11\n",
      "this: 12\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "x = vectorizer.fit_transform(sentences)\n",
    "\n",
    "sorted_vocab_by_index = sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])\n",
    "for word, index in sorted_vocab_by_index:\n",
    "    print(f\"{word}: {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.4472136 , 0.        , 0.4472136 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.4472136 , 0.4472136 , 0.4472136 ],\n",
       "       [0.57735027, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.        , 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.4472136 , 0.        , 0.4472136 ,\n",
       "        0.        , 0.4472136 , 0.4472136 , 0.4472136 , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TF-IDF***\n",
    "\n",
    "- Term Frequency Inverse Document frequency\n",
    "- we try to count the TF and IDF of each words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***word to vector***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'is', 'for', 'testing', 'statement'],\n",
       " ['I', 'love', 'playing', 'cricket'],\n",
       " ['I', 'have', 'played', 'lot', 'of', 'matches']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    'This is for testing statement',\n",
    "    'I love playing cricket',\n",
    "    'I have played lot of matches'\n",
    "]\n",
    "\n",
    "# Tokenize each sentence\n",
    "tokenized_sentences = [re.findall(r'\\b\\w+\\b', sentence) for sentence in sentences]\n",
    "\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x176163850>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sending this token to word2vec mode;\n",
    "\n",
    "model = Word2Vec(tokenized_sentences,vector_size=100,window=5,min_count=1,workers=4,sg=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.13227147e-03, -4.45733406e-03, -1.06835726e-03,  1.00636482e-03,\n",
       "       -1.91113955e-04,  1.14817743e-03,  6.11386076e-03, -2.02715401e-05,\n",
       "       -3.24596534e-03, -1.51072862e-03,  5.89729892e-03,  1.51410222e-03,\n",
       "       -7.24261976e-04,  9.33324732e-03, -4.92128357e-03, -8.38409644e-04,\n",
       "        9.17541143e-03,  6.74942741e-03,  1.50285603e-03, -8.88256077e-03,\n",
       "        1.14874600e-03, -2.28825561e-03,  9.36823711e-03,  1.20992784e-03,\n",
       "        1.49006362e-03,  2.40640994e-03, -1.83600665e-03, -4.99963388e-03,\n",
       "        2.32429506e-04, -2.01418041e-03,  6.60093315e-03,  8.94012302e-03,\n",
       "       -6.74754381e-04,  2.97701475e-03, -6.10765442e-03,  1.69932481e-03,\n",
       "       -6.92623248e-03, -8.69402662e-03, -5.90020278e-03, -8.95647518e-03,\n",
       "        7.27759488e-03, -5.77203138e-03,  8.27635173e-03, -7.24354526e-03,\n",
       "        3.42167495e-03,  9.67499893e-03, -7.78544787e-03, -9.94505733e-03,\n",
       "       -4.32914635e-03, -2.68313056e-03, -2.71289347e-04, -8.83155130e-03,\n",
       "       -8.61755759e-03,  2.80021061e-03, -8.20640661e-03, -9.06933658e-03,\n",
       "       -2.34046578e-03, -8.63180775e-03, -7.05664977e-03, -8.40115082e-03,\n",
       "       -3.01328895e-04, -4.56429832e-03,  6.62717456e-03,  1.52716041e-03,\n",
       "       -3.34147573e-03,  6.10897178e-03, -6.01328490e-03, -4.65616956e-03,\n",
       "       -7.20750913e-03, -4.33658017e-03, -1.80932996e-03,  6.48964290e-03,\n",
       "       -2.77039292e-03,  4.91896737e-03,  6.90444233e-03, -7.46370573e-03,\n",
       "        4.56485013e-03,  6.12697843e-03, -2.95447465e-03,  6.62502181e-03,\n",
       "        6.12587947e-03, -6.44348515e-03, -6.76455162e-03,  2.53895880e-03,\n",
       "       -1.62381888e-03, -6.06512791e-03,  9.49920900e-03, -5.13014663e-03,\n",
       "       -6.55409694e-03, -1.19885204e-04, -2.70142802e-03,  4.44400299e-04,\n",
       "       -3.53745813e-03, -4.19330609e-04, -7.08615757e-04,  8.22820642e-04,\n",
       "        8.19481723e-03, -5.73670724e-03, -1.65952800e-03,  5.57160750e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['cricket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
